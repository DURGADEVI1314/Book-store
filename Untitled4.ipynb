{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2lEh87lY+o7e5VGTkrStC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DURGADEVI1314/Book-store/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2_jkZ7bZ0LeZ",
        "outputId": "610f5100-cd09-4f0d-f5ec-480ea651441c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 19) (<ipython-input-4-ac0616f25dc6>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ac0616f25dc6>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    \"execution_count\":'' null'\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 19)\n"
          ]
        }
      ],
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\":'' null'\n",
        "      \"metadata\": {\n",
        "        \"id\": \"n2_Ivn4Tjv0M\"\n",
        "      },\n",
        "      \"outputs\": [],\n",
        "      \"source\": [\n",
        "        \"import pandas as pd\\n\",\n",
        "        \"\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"\\n\",\n",
        "        \"import re\\n\",\n",
        "        \"\\n\",\n",
        "        \"import nltk\\n\",\n",
        "        \"\\n\",\n",
        "        \"nltk.download('stopwords')\\n\",\n",
        "        \"\\n\",\n",
        "        \"from nltk.tokenize import word tokenize\\n\",\n",
        "        \"\\n\",\n",
        "        \"from nltk.corpus import stopwords\\n\",\n",
        "        \"\\n\",\n",
        "        \"from nltk.stem import WordNetLemmatizer\\n\",\n",
        "        \"\\n\",\n",
        "        \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
        "        \"\\n\",\n",
        "        \"from sklearn.decomposition import LatentDirichletAllocation\\n\",\n",
        "        \"\\n\",\n",
        "        \"from textblob import TextBlob\\n\",\n",
        "        \"\\n\",\n",
        "        \"nltk.download('punkt')\\n\",\n",
        "        \"\\n\",\n",
        "        \"nltk.download('wordnet')\\n\",\n",
        "        \"\\n\",\n",
        "        \"nltk.download(\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Load data\\n\",\n",
        "        \"\\n\",\n",
        "        \"car_reviews = pd.read_csv(\\\"/content/Scraped_Car_Review_tesla.csv\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Data preprocessing\\n\",\n",
        "        \"\\n\",\n",
        "        \"def preprocess text(text):\\n\",\n",
        "        \"\\n\",\n",
        "        \"text re.sub(r'\\\\d+', \\\", text) # Remove numbers\\n\",\n",
        "        \"\\n\",\n",
        "        \"text text lower() # Convert to lowercase\\n\",\n",
        "        \"\\n\",\n",
        "        \"text = re.sub(r'\\\\s+','', text) # Remove extra whitespaces\\n\",\n",
        "        \"\\n\",\n",
        "        \"text = re.sub(r<[^>]+>', \\\", text) # Remove HTML tags\\n\",\n",
        "        \"\\n\",\n",
        "        \"text = re.sub(r\\\\s+', '', text) # Remove extra whitespaces againreturn text\\n\",\n",
        "        \"\\n\",\n",
        "        \"car_reviews = car_reviews.rename(columns={'review':'Review'})\\n\",\n",
        "        \"\\n\",\n",
        "        \"#Tokenization, lemmatization, and stopword removal\\n\",\n",
        "        \"\\n\",\n",
        "        \"stop_words = set(stopwords words('english'))\\n\",\n",
        "        \"\\n\",\n",
        "        \"lemmatizer = WordNetLemmatizer()\\n\",\n",
        "        \"\\n\",\n",
        "        \"def tokenize_lemmatize(text):\\n\",\n",
        "        \"\\n\",\n",
        "        \"tokens word tokenize(text)\\n\",\n",
        "        \"\\n\",\n",
        "        \"tokens = [lemmatizer. lemmatize(word) for word in tokens if word not in\\n\",\n",
        "        \"\\n\",\n",
        "        \"stop_words]\\n\",\n",
        "        \"\\n\",\n",
        "        \"return tokens\\n\",\n",
        "        \"\\n\",\n",
        "        \"car_reviews['tokens'] = car_reviews['Review'].apply(tokenize_lemmatize)\\n\",\n",
        "        \"\\n\",\n",
        "        \"#Sentiment analysis using TextBlob\\n\",\n",
        "        \"\\n\",\n",
        "        \"def get sentiment(text):\\n\",\n",
        "        \"\\n\",\n",
        "        \"analysis TextBlob(text)\\n\",\n",
        "        \"\\n\",\n",
        "        \"return 'positive' if analysis sentiment polarity > 0 else 'negative'\\n\",\n",
        "        \"\\n\",\n",
        "        \"car_reviews['sentiment'] = car_reviews['Review'].apply(get_sentiment)\\n\",\n",
        "        \"\\n\",\n",
        "        \"#Topic modeling using Latent Dirichlet Allocation (LDA)\\n\",\n",
        "        \"\\n\",\n",
        "        \"def lda_topic_modeling(reviews, num_topics=5):\\n\",\n",
        "        \"\\n\",\n",
        "        \"vectorizer =TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\\n\",\n",
        "        \"\\n\",\n",
        "        \"tfidf vectorizer.fit transform(reviews)Ida_model LatentDirichletAllocation(n_components=num_topics,\\n\",\n",
        "        \"\\n\",\n",
        "        \"max_iter=10, learning_method='online', random_state=42)\\n\",\n",
        "        \"\\n\",\n",
        "        \"Ida_model, fit(tfidf)\\n\",\n",
        "        \"\\n\",\n",
        "        \"return lda model, vectorizer\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Perform topic modeling on positive and negative reviews separately\\n\",\n",
        "        \"\\n\",\n",
        "        \"positive_reviews = car_reviews[car_reviews['sentiment'] 'positive']['Review']\\n\",\n",
        "        \"\\n\",\n",
        "        \"negative_reviews = car_reviews[car_reviews['sentiment'] = 'negative']['Review']\\n\",\n",
        "        \"\\n\",\n",
        "        \"positive_lda_model, positive_vectorizer = Ida_topic_modeling(positive_reviews)\\n\",\n",
        "        \"\\n\",\n",
        "        \"negative_Ida_model, negative_vectorizer = Ida topic modeling(negative reviews)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Display topics for positive and negative reviews\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"Topics for Positive Reviews:\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"for index, topic in enumerate(positive_Ida_model.components):\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"Topic (index+1}:\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"top words indices = topic argsort()[-10:]\\n\",\n",
        "        \"\\n\",\n",
        "        \"top_words = [positive_vectorizer.get_feature_names_out()[i] for i in\\n\",\n",
        "        \"\\n\",\n",
        "        \"top_words_indices]\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(top_words)\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nTopics for Negative Reviews:\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"for index, topic in enumerate(negative_Ida_model.components_):\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f'Topic {index+1}:\\\")top_words_indices = topic.argsort()[-10:]\\n\",\n",
        "        \"\\n\",\n",
        "        \"top_words = [negative_vectorizer.get_feature_names_out()[i] for i in\\n\",\n",
        "        \"\\n\",\n",
        "        \"top_words_indices]\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(top_words)\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load data\n",
        "car_reviews = pd.read_csv(\"/content/Scraped_Car_Review_tesla.csv\")\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces again\n",
        "    return text\n",
        "\n",
        "car_reviews = car_reviews.rename(columns={'review': 'Review'})\n",
        "\n",
        "# Tokenization, lemmatization, and stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_lemmatize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "car_reviews['tokens'] = car_reviews['Review'].apply(tokenize_lemmatize)\n",
        "\n",
        "# Sentiment analysis using TextBlob\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return 'positive' if analysis.sentiment.polarity > 0 else 'negative'\n",
        "\n",
        "car_reviews['sentiment'] = car_reviews['Review'].apply(get_sentiment)\n",
        "\n",
        "# Topic modeling using Latent Dirichlet Allocation (LDA)\n",
        "def lda_topic_modeling(reviews, num_topics=5):\n",
        "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    tfidf = vectorizer.fit_transform(reviews)\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online', random_state=42)\n",
        "    lda_model.fit(tfidf)\n",
        "    return lda_model, vectorizer\n",
        "\n",
        "# Perform topic modeling on positive and negative reviews separately\n",
        "positive_reviews = car_reviews[car_reviews['sentiment'] == 'positive']['Review']\n",
        "negative_reviews = car_reviews[car_reviews['sentiment'] == 'negative']['Review']\n",
        "\n",
        "positive_lda_model, positive_vectorizer = lda_topic_modeling(positive_reviews)\n",
        "negative_lda_model, negative_vectorizer = lda_topic_modeling(negative_reviews)\n",
        "\n",
        "# Display topics for positive and negative reviews\n",
        "print(\"Topics for Positive Reviews:\")\n",
        "for index, topic in enumerate(positive_lda_model.components_):\n",
        "    print(f\"Topic {index + 1}:\")\n",
        "    top_words_indices = topic.argsort()[-10:]\n",
        "    top_words = [positive_vectorizer.get_feature_names_out()[i] for i in top_words_indices]\n",
        "    print(top_words)\n",
        "\n",
        "print(\"\\nTopics for Negative Reviews:\")\n",
        "for index, topic in enumerate(negative_lda_model.components_):\n",
        "    print(f\"Topic {index + 1}:\")\n",
        "    top_words_indices = topic.argsort()[-10:]\n",
        "    top_words = [negative_vectorizer.get_feature_names_out()[i] for i in top_words_indices]\n",
        "    print(top_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "kc8mBx7P1UMU",
        "outputId": "dce5a501-3414-4b25-f0b5-297001cf5a8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Scraped_Car_Review_tesla.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a9d906b5764c>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcar_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Scraped_Car_Review_tesla.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Scraped_Car_Review_tesla.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to load existing expenses from the file\n",
        "def load_expenses(filename=\"expenses.json\"):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    return []\n",
        "\n",
        "# Function to save expenses to the file\n",
        "def save_expenses(expenses, filename=\"expenses.json\"):\n",
        "    with open(filename, \"w\") as file:\n",
        "        json.dump(expenses, file, indent=4)\n",
        "\n",
        "# Function to add an expense\n",
        "def add_expense(expenses):\n",
        "    amount = float(input(\"Enter the amount spent: \"))\n",
        "    category = input(\"Enter the expense category (e.g., food, transport, entertainment): \")\n",
        "    description = input(\"Enter a short description: \")\n",
        "    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    expense = {\n",
        "        \"amount\": amount,\n",
        "        \"category\": category,\n",
        "        \"description\": description,\n",
        "        \"date\": date\n",
        "    }\n",
        "    expenses.append(expense)\n",
        "    save_expenses(expenses)\n",
        "    print(\"Expense added successfully.\")\n",
        "\n",
        "# Function to view all expenses\n",
        "def view_expenses(expenses):\n",
        "    if not expenses:\n",
        "        print(\"No expenses recorded.\")\n",
        "    else:\n",
        "        for i, expense in enumerate(expenses, 1):\n",
        "            print(f\"{i}. {expense['amount']} - {expense['category']} - {expense['description']} - {expense['date']}\")\n",
        "\n",
        "# Function to view a summary of expenses by category\n",
        "def expense_summary(expenses):\n",
        "    category_summary = {}\n",
        "    for expense in expenses:\n",
        "        category = expense[\"category\"]\n",
        "        amount = expense[\"amount\"]\n",
        "        if category in category_summary:\n",
        "            category_summary[category] += amount\n",
        "        else:\n",
        "            category_summary[category] = amount\n",
        "\n",
        "    if category_summary:\n",
        "        for category, total in category_summary.items():\n",
        "            print(f\"{category}: {total}\")\n",
        "    else:\n",
        "        print(\"No expenses recorded.\")\n",
        "\n",
        "# Function to view the total of all expenses\n",
        "def total_expenses(expenses):\n",
        "    total = sum(expense[\"amount\"] for expense in expenses)\n",
        "    print(f\"Total Expenses: {total}\")\n",
        "\n",
        "# Main function to run the app\n",
        "def main():\n",
        "    expenses = load_expenses()\n",
        "    while True:\n",
        "        print(\"\\nExpense Tracker\")\n",
        "        print(\"1. Add Expense\")\n",
        "        print(\"2. View Expenses\")\n",
        "        print(\"3. View Expense Summary by Category\")\n",
        "        print(\"4. View Total Expenses\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            add_expense(expenses)\n",
        "        elif choice == \"2\":\n",
        "            view_expenses(expenses)\n",
        "        elif choice == \"3\":\n",
        "            expense_summary(expenses)\n",
        "        elif choice == \"4\":\n",
        "            total_expenses(expenses)\n",
        "        elif choice == \"5\":\n",
        "            print(\"Exiting the app.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice, please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y42ieoCZ3HLV",
        "outputId": "10df08d2-2268-4635-d8c9-9f7ca8f1ff98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Expense Tracker\n",
            "1. Add Expense\n",
            "2. View Expenses\n",
            "3. View Expense Summary by Category\n",
            "4. View Total Expenses\n",
            "5. Exit\n",
            "Enter your choice: 2\n",
            "No expenses recorded.\n",
            "\n",
            "Expense Tracker\n",
            "1. Add Expense\n",
            "2. View Expenses\n",
            "3. View Expense Summary by Category\n",
            "4. View Total Expenses\n",
            "5. Exit\n"
          ]
        }
      ]
    }
  ]
}